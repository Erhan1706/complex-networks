{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KuaiRec Dataset Exploration\n",
    "\n",
    "## Theoretical Framework: Complex Networks & Data-Driven Modeling\n",
    "\n",
    "This notebook explores the KuaiRec dataset through the lens of **complex networks theory** and **data-driven modeling**.\n",
    "\n",
    "### What is a Complex Network?\n",
    "\n",
    "A **complex network** is a graph with non-trivial topological features that don't occur in simple networks (like lattices or random graphs). Examples include:\n",
    "- Social networks (people connected by friendships)\n",
    "- Information networks (web pages connected by hyperlinks)\n",
    "- Recommendation networks (users connected through shared interests)\n",
    "\n",
    "### Two Core Approaches in This Course\n",
    "\n",
    "#### 1. Network Characterization\n",
    "**Goal**: Understand the structure and properties of networks\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Degree Distribution**: How many connections does each node have? In social networks, most people have few friends, but some have many (power-law distribution)\n",
    "- **Clustering**: Do friends of your friends tend to be your friends? (Triangles in the network)\n",
    "- **Path Lengths**: How many steps to get from one user to another?\n",
    "- **Centrality**: Who are the most important/influential nodes?\n",
    "  - *Degree centrality*: Simply how many connections\n",
    "  - *Betweenness centrality*: How often a node lies on shortest paths between others\n",
    "  - *Eigenvector centrality*: Being connected to important nodes makes you important\n",
    "- **Community Detection**: Are there clusters of tightly connected users?\n",
    "\n",
    "**Why this matters for KuaiRec**: The social network structure can influence what videos people watch. Users connected to many others might spread content faster (diffusion).\n",
    "\n",
    "#### 2. Data-Driven Modeling\n",
    "**Goal**: Build predictive models based on data patterns\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Regression Models**: Predict a continuous outcome (e.g., watch time) from features\n",
    "  - *LASSO regression*: Automatically selects important features by pushing irrelevant coefficients to zero\n",
    "  - *Ridge regression*: Prevents overfitting by penalizing large coefficients\n",
    "- **Feature Engineering**: Creating informative variables from raw data\n",
    "  - Network features: degree, centrality, clustering coefficient of each user\n",
    "  - Content features: video tags, duration, popularity\n",
    "  - Temporal features: time of day, day of week\n",
    "- **Model Interpretation**: What factors actually drive behavior?\n",
    "\n",
    "**Why this matters for KuaiRec**: We want to understand what predicts watch time - is it the user's social position? Video popularity? Content type?\n",
    "\n",
    "---\n",
    "\n",
    "## Our Research Question\n",
    "\n",
    "**Primary Focus**: What factors influence user watch time and engagement patterns?\n",
    "\n",
    "We'll combine both approaches:\n",
    "1. **Network Characterization**: Analyze the social network structure\n",
    "2. **Data-Driven Modeling**: Build predictive models using network features + content features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Network analysis\n",
    "import networkx as nx\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "The KuaiRec dataset contains:\n",
    "- **User-item interaction matrix**: Which users watched which videos and for how long\n",
    "- **Social network**: Follower relationships between users\n",
    "- **Video features**: Content tags, duration, popularity metrics\n",
    "- **User features**: Demographics, activity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load KuaiRec dataset\nprint(\"Loading KuaiRec dataset...\")\n\n# Start with small_matrix for faster initial exploration\n# Switch to big_matrix once you're ready for full analysis\ninteractions = pd.read_csv('../data/raw/small_matrix.csv')\nprint(f\"✓ Loaded interactions: {interactions.shape[0]:,} rows, {interactions.shape[1]} columns\")\n\n# Social network (friend connections)\nsocial_network = pd.read_csv('../data/raw/social_network.csv')\nprint(f\"✓ Loaded social network: {social_network.shape[0]:,} users\")\n\n# Video features (categories/tags)\nvideo_features = pd.read_csv('../data/raw/item_categories.csv')\nprint(f\"✓ Loaded video features: {video_features.shape[0]:,} videos\")\n\n# User features (demographics, activity levels)\nuser_features = pd.read_csv('../data/raw/user_features.csv')\nprint(f\"✓ Loaded user features: {user_features.shape[0]:,} users\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Dataset Overview\")\nprint(\"=\"*60)\nprint(f\"Total interactions: {interactions.shape[0]:,}\")\nprint(f\"Unique users: {interactions['user_id'].nunique():,}\")\nprint(f\"Unique videos: {interactions['video_id'].nunique():,}\")\nprint(f\"Date range: {interactions['date'].min()} to {interactions['date'].max()}\")\n\n# Display sample of interaction data\nprint(\"\\nSample interaction data:\")\nprint(interactions.head())\n\nprint(\"\\nInteraction columns:\")\nprint(interactions.columns.tolist())\n\nprint(\"\\nUser feature columns:\")\nprint(user_features.columns.tolist())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Characterization\n",
    "\n",
    "### Theory: Why Analyze the Social Network?\n",
    "\n",
    "**Social influence hypothesis**: People's behavior (including video watching) is influenced by their social connections.\n",
    "\n",
    "**Key questions**:\n",
    "- Do central users watch more content?\n",
    "- Do users in dense communities have similar watch patterns?\n",
    "- How does content diffuse through the network?\n",
    "\n",
    "### Network Metrics to Compute\n",
    "\n",
    "1. **Basic Statistics**\n",
    "   - Number of nodes (users) and edges (connections)\n",
    "   - Degree distribution\n",
    "   - Density: How connected is the network?\n",
    "\n",
    "2. **Structural Properties**\n",
    "   - Clustering coefficient: Do friends cluster together?\n",
    "   - Average path length: How many hops between users?\n",
    "   - Connected components: Is the network fragmented?\n",
    "\n",
    "3. **Centrality Measures** (for each user)\n",
    "   - Degree: Number of connections\n",
    "   - Betweenness: Bridge between communities\n",
    "   - Eigenvector: Connected to important people\n",
    "   - PageRank: Like Google's algorithm for web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build network from social connections\nprint(\"Building social network graph...\")\n\n# Parse friend_list (stored as string representation of list)\nimport ast\n\nedges = []\nfor _, row in social_network.iterrows():\n    user_id = row['user_id']\n    friend_list = ast.literal_eval(row['friend_list'])\n    for friend_id in friend_list:\n        edges.append((user_id, friend_id))\n\nprint(f\"Total edges extracted: {len(edges):,}\")\n\n# Create directed graph (following relationships)\nG = nx.DiGraph()\nG.add_edges_from(edges)\n\nprint(f\"\\nNetwork Statistics:\")\nprint(f\"  Number of users (nodes): {G.number_of_nodes():,}\")\nprint(f\"  Number of connections (edges): {G.number_of_edges():,}\")\nprint(f\"  Network density: {nx.density(G):.6f}\")\nprint(f\"  Is connected: {nx.is_weakly_connected(G)}\")\n\n# For undirected metrics, create undirected version\nG_undirected = G.to_undirected()\nprint(f\"  Connected components: {nx.number_connected_components(G_undirected)}\")\n\n# Degree distribution\ndegrees = dict(G.degree())\ndegree_values = list(degrees.values())\n\nplt.figure(figsize=(14, 5))\n\n# Linear scale\nplt.subplot(1, 2, 1)\nplt.hist(degree_values, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Degree (Number of Connections)')\nplt.ylabel('Frequency')\nplt.title('Degree Distribution (Linear Scale)')\nplt.grid(alpha=0.3)\n\n# Log-log scale (to check for power law)\nplt.subplot(1, 2, 2)\nplt.hist(degree_values, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Degree (Number of Connections)')\nplt.ylabel('Frequency')\nplt.title('Degree Distribution (Log-Log Scale)')\nplt.yscale('log')\nplt.xscale('log')\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nDegree Statistics:\")\nprint(f\"  Mean degree: {np.mean(degree_values):.2f}\")\nprint(f\"  Median degree: {np.median(degree_values):.2f}\")\nprint(f\"  Max degree: {np.max(degree_values)}\")\nprint(f\"  Min degree: {np.min(degree_values)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation Guide\n",
    "\n",
    "**What to look for**:\n",
    "- **Power-law degree distribution**: A few highly connected users (influencers), many with few connections\n",
    "- **High clustering**: Indicates community structure\n",
    "- **Small world property**: Short average path length despite large network size\n",
    "\n",
    "**Scale-free networks**: If degree follows power law P(k) ~ k^(-γ), the network is scale-free (common in social networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate centrality measures for each user\n# These become FEATURES for our predictive model\nprint(\"Computing centrality measures...\")\nprint(\"(This may take a minute for larger networks)\")\n\n# Degree centrality (fast)\ndegree_centrality = nx.degree_centrality(G)\nprint(\"✓ Degree centrality computed\")\n\n# PageRank (fast, good for directed graphs)\npagerank = nx.pagerank(G, max_iter=100)\nprint(\"✓ PageRank computed\")\n\n# Betweenness centrality (slower - use sampling for large networks)\n# For small network, compute exactly; for large, use approximation\nif G.number_of_nodes() < 5000:\n    betweenness_centrality = nx.betweenness_centrality(G)\n    print(\"✓ Betweenness centrality computed (exact)\")\nelse:\n    # Use sampling for large networks\n    sample_k = min(100, G.number_of_nodes())\n    betweenness_centrality = nx.betweenness_centrality(G, k=sample_k)\n    print(f\"✓ Betweenness centrality computed (sampled, k={sample_k})\")\n\n# Clustering coefficient (use undirected for this metric)\nclustering_coef = nx.clustering(G_undirected)\nprint(\"✓ Clustering coefficient computed\")\n\n# Create dataframe of network features\nnetwork_features = pd.DataFrame({\n    'user_id': list(degree_centrality.keys()),\n    'degree_centrality': list(degree_centrality.values()),\n    'betweenness_centrality': [betweenness_centrality[node] for node in degree_centrality.keys()],\n    'pagerank': [pagerank[node] for node in degree_centrality.keys()],\n    'clustering_coef': [clustering_coef[node] for node in degree_centrality.keys()]\n})\n\nprint(f\"\\nNetwork features created for {len(network_features)} users\")\nprint(\"\\nSample network features:\")\nprint(network_features.head())\n\n# Visualize centrality distributions\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\naxes[0, 0].hist(network_features['degree_centrality'], bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].set_xlabel('Degree Centrality')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('Degree Centrality Distribution')\naxes[0, 0].grid(alpha=0.3)\n\naxes[0, 1].hist(network_features['pagerank'], bins=50, edgecolor='black', alpha=0.7)\naxes[0, 1].set_xlabel('PageRank')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('PageRank Distribution')\naxes[0, 1].grid(alpha=0.3)\n\naxes[1, 0].hist(network_features['betweenness_centrality'], bins=50, edgecolor='black', alpha=0.7)\naxes[1, 0].set_xlabel('Betweenness Centrality')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Betweenness Centrality Distribution')\naxes[1, 0].grid(alpha=0.3)\n\naxes[1, 1].hist(network_features['clustering_coef'], bins=50, edgecolor='black', alpha=0.7)\naxes[1, 1].set_xlabel('Clustering Coefficient')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Clustering Coefficient Distribution')\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### Understanding Watch Time Patterns\n",
    "\n",
    "**Watch ratio** = (time watched) / (video duration)\n",
    "- 1.0 = watched entire video\n",
    "- <1.0 = skipped parts\n",
    "- >1.0 = rewatched parts\n",
    "\n",
    "This is our **target variable** for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore watch time distribution\nprint(\"Watch Ratio Statistics:\")\nprint(f\"  Mean: {interactions['watch_ratio'].mean():.3f}\")\nprint(f\"  Median: {interactions['watch_ratio'].median():.3f}\")\nprint(f\"  Std: {interactions['watch_ratio'].std():.3f}\")\nprint(f\"  Min: {interactions['watch_ratio'].min():.3f}\")\nprint(f\"  Max: {interactions['watch_ratio'].max():.3f}\")\nprint(f\"\\nInterpretation:\")\nprint(\"  watch_ratio = 1.0 means watched entire video\")\nprint(\"  watch_ratio < 1.0 means skipped/stopped early\")\nprint(\"  watch_ratio > 1.0 means rewatched or looped\")\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Watch ratio distribution\naxes[0, 0].hist(interactions['watch_ratio'], bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].set_xlabel('Watch Ratio')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('Distribution of Watch Ratios')\naxes[0, 0].axvline(1.0, color='red', linestyle='--', label='Complete watch')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\n\n# Watch ratio vs video duration\nsample_size = min(10000, len(interactions))\nsample = interactions.sample(sample_size, random_state=42)\naxes[0, 1].scatter(sample['video_duration'], sample['watch_ratio'], alpha=0.1, s=1)\naxes[0, 1].set_xlabel('Video Duration (ms)')\naxes[0, 1].set_ylabel('Watch Ratio')\naxes[0, 1].set_title(f'Watch Ratio vs Video Duration (n={sample_size:,})')\naxes[0, 1].grid(alpha=0.3)\n\n# Video duration distribution\naxes[1, 0].hist(interactions['video_duration'] / 1000, bins=50, edgecolor='black', alpha=0.7)\naxes[1, 0].set_xlabel('Video Duration (seconds)')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Distribution of Video Durations')\naxes[1, 0].grid(alpha=0.3)\n\n# Play duration distribution\naxes[1, 1].hist(interactions['play_duration'] / 1000, bins=50, edgecolor='black', alpha=0.7)\naxes[1, 1].set_xlabel('Play Duration (seconds)')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Distribution of Actual Watch Time')\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# User and video activity\nprint(f\"\\nUser Activity:\")\ninteractions_per_user = interactions.groupby('user_id').size()\nprint(f\"  Mean interactions per user: {interactions_per_user.mean():.1f}\")\nprint(f\"  Median interactions per user: {interactions_per_user.median():.1f}\")\nprint(f\"  Max interactions by single user: {interactions_per_user.max()}\")\n\nprint(f\"\\nVideo Popularity:\")\nviews_per_video = interactions.groupby('video_id').size()\nprint(f\"  Mean views per video: {views_per_video.mean():.1f}\")\nprint(f\"  Median views per video: {views_per_video.median():.1f}\")\nprint(f\"  Max views for single video: {views_per_video.max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### Theory: What Features Matter?\n",
    "\n",
    "To predict watch time, we need features that capture:\n",
    "\n",
    "1. **User characteristics**\n",
    "   - Network position (centrality measures)\n",
    "   - Activity level (total videos watched)\n",
    "   - Average watch behavior\n",
    "\n",
    "2. **Video characteristics**\n",
    "   - Duration\n",
    "   - Popularity (how many views)\n",
    "   - Content tags (comedy, education, etc.)\n",
    "\n",
    "3. **Social influence**\n",
    "   - How many of user's friends watched this video?\n",
    "   - Average rating by friends\n",
    "\n",
    "4. **Temporal patterns**\n",
    "   - Time of day\n",
    "   - Day of week\n",
    "   - Sequence position (1st video of session vs 10th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feature engineering\n",
    "\n",
    "# User-level features\n",
    "# user_activity = interactions.groupby('user_id').agg({\n",
    "#     'watch_ratio': ['mean', 'std', 'count'],\n",
    "#     'video_duration': 'mean'\n",
    "# })\n",
    "\n",
    "# Video popularity\n",
    "# video_popularity = interactions.groupby('video_id').agg({\n",
    "#     'watch_ratio': 'mean',\n",
    "#     'user_id': 'count'  # number of views\n",
    "# })\n",
    "\n",
    "# Merge all features\n",
    "# full_data = interactions.merge(network_features, on='user_id')\n",
    "# full_data = full_data.merge(video_features, on='video_id')\n",
    "# full_data = full_data.merge(user_activity, on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictive Modeling\n",
    "\n",
    "### Theory: LASSO Regression\n",
    "\n",
    "**Why LASSO?**\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) minimizes:\n",
    "\n",
    "$$\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "- First term: Prediction error (standard regression)\n",
    "- Second term: L1 penalty on coefficients\n",
    "\n",
    "**Key advantage**: Automatic feature selection\n",
    "- Irrelevant features get coefficient = 0\n",
    "- Helps identify what TRULY matters for watch time\n",
    "\n",
    "**λ (lambda)**: Controls regularization strength\n",
    "- Small λ: More features retained, risk of overfitting\n",
    "- Large λ: Fewer features, simpler model\n",
    "- Choose via cross-validation\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "After fitting LASSO:\n",
    "- **Non-zero coefficients**: Important features\n",
    "- **Sign of coefficient**: Positive = increases watch time, Negative = decreases\n",
    "- **Magnitude**: Strength of effect (after standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# feature_columns = ['degree_centrality', 'betweenness_centrality', 'pagerank',\n",
    "#                   'video_duration', 'video_popularity', ...]\n",
    "\n",
    "# X = full_data[feature_columns]\n",
    "# y = full_data['watch_ratio']\n",
    "\n",
    "# Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (important for LASSO)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit LASSO\n",
    "# lasso = Lasso(alpha=0.01)  # alpha = λ\n",
    "# lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model performance\n",
    "# train_score = lasso.score(X_train_scaled, y_train)\n",
    "# test_score = lasso.score(X_test_scaled, y_test)\n",
    "# print(f\"Train R²: {train_score:.3f}\")\n",
    "# print(f\"Test R²: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# coefficients = pd.DataFrame({\n",
    "#     'feature': feature_columns,\n",
    "#     'coefficient': lasso.coef_\n",
    "# })\n",
    "# coefficients = coefficients[coefficients['coefficient'] != 0].sort_values('coefficient', ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(coefficients['feature'], coefficients['coefficient'])\n",
    "# plt.xlabel('LASSO Coefficient')\n",
    "# plt.title('Important Features for Predicting Watch Time')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Network Influence on Watch Behavior\n",
    "\n",
    "### Theory: Social Influence and Diffusion\n",
    "\n",
    "**Question**: Does network position affect what/how much people watch?\n",
    "\n",
    "**Hypotheses to test**:\n",
    "1. Users with higher centrality watch more content\n",
    "2. Users in dense communities have correlated watch patterns\n",
    "3. Content diffuses through network connections\n",
    "\n",
    "**Diffusion models**:\n",
    "- **Threshold models**: Users adopt after k friends do\n",
    "- **Cascade models**: Probabilistic spread through edges\n",
    "- **Exposure models**: More exposures = higher adoption probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between network position and behavior\n",
    "\n",
    "# Total watch time per user\n",
    "# user_total_watch = interactions.groupby('user_id')['watch_ratio'].sum()\n",
    "\n",
    "# Merge with centrality\n",
    "# analysis_df = network_features.copy()\n",
    "# analysis_df['total_watch'] = analysis_df['user_id'].map(user_total_watch)\n",
    "\n",
    "# Scatter plots\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# axes[0].scatter(analysis_df['degree_centrality'], analysis_df['total_watch'], alpha=0.3)\n",
    "# axes[0].set_xlabel('Degree Centrality')\n",
    "# axes[0].set_ylabel('Total Watch Ratio')\n",
    "# axes[0].set_title('Degree vs Watch Behavior')\n",
    "\n",
    "# axes[1].scatter(analysis_df['betweenness_centrality'], analysis_df['total_watch'], alpha=0.3)\n",
    "# axes[1].set_xlabel('Betweenness Centrality')\n",
    "# axes[1].set_ylabel('Total Watch Ratio')\n",
    "# axes[1].set_title('Betweenness vs Watch Behavior')\n",
    "\n",
    "# axes[2].scatter(analysis_df['pagerank'], analysis_df['total_watch'], alpha=0.3)\n",
    "# axes[2].set_xlabel('PageRank')\n",
    "# axes[2].set_ylabel('Total Watch Ratio')\n",
    "# axes[2].set_title('PageRank vs Watch Behavior')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps and Research Questions\n",
    "\n",
    "### Questions to Explore\n",
    "\n",
    "1. **Feature importance**: Which factors most strongly predict watch time?\n",
    "   - Network position? Content features? Social influence?\n",
    "\n",
    "2. **Community structure**: Do communities have different watch patterns?\n",
    "   - Use community detection (Louvain, Girvan-Newman)\n",
    "   - Compare watch behavior across communities\n",
    "\n",
    "3. **Temporal dynamics**: How does engagement change over time?\n",
    "   - Time series analysis\n",
    "   - Session-level patterns\n",
    "\n",
    "4. **Diffusion analysis**: How do videos spread through the network?\n",
    "   - Track which users watch which videos when\n",
    "   - Model as contagion process\n",
    "\n",
    "5. **Advanced modeling**:\n",
    "   - Graph Neural Networks (GNNs) to incorporate network structure\n",
    "   - Recommender systems\n",
    "   - Causal inference (does network position CAUSE different behavior?)\n",
    "\n",
    "### Potential Findings\n",
    "\n",
    "**If network features are important**: Social influence matters for engagement\n",
    "**If content features dominate**: Individual preferences override social effects\n",
    "**If both matter**: Complex interaction between social and content factors\n",
    "\n",
    "### Deliverables for Team Discussion\n",
    "\n",
    "1. Network characterization summary\n",
    "2. Initial predictive model results\n",
    "3. Key insights and surprising findings\n",
    "4. Proposed research directions for final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Complex Networks**: Barabási, A. L., & Albert, R. (1999). Emergence of scaling in random networks. Science.\n",
    "- **Social Influence**: Aral, S., & Walker, D. (2012). Identifying influential and susceptible members of social networks. Science.\n",
    "- **LASSO Regression**: Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. JRSS-B.\n",
    "- **Network Diffusion**: Kempe, D., Kleinberg, J., & Tardos, É. (2003). Maximizing the spread of influence through a social network. KDD.\n",
    "\n",
    "### Tools\n",
    "\n",
    "- **NetworkX**: https://networkx.org/\n",
    "- **scikit-learn**: https://scikit-learn.org/\n",
    "- **KuaiRec Dataset**: https://chongminggao.github.io/KuaiRec/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}