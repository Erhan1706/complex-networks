{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIR Spreading on Similarity Network\n",
    "\n",
    "Measure node influence by final outbreak size in weighted similarity network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and build network\n",
    "interactions = pd.read_csv('../data/raw/small_matrix.csv')\n",
    "\n",
    "user_video_matrix = interactions.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='video_id',\n",
    "    values='watch_ratio',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "similarity_matrix = cosine_similarity(user_video_matrix.values)\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=user_video_matrix.index,\n",
    "    columns=user_video_matrix.index\n",
    ")\n",
    "\n",
    "G = nx.Graph()\n",
    "users = user_video_matrix.index.tolist()\n",
    "G.add_nodes_from(users)\n",
    "\n",
    "for i, user_i in enumerate(users):\n",
    "    for j, user_j in enumerate(users[i+1:], start=i+1):\n",
    "        sim = similarity_df.loc[user_i, user_j]\n",
    "        G.add_edge(user_i, user_j, weight=sim)\n",
    "\n",
    "print(f\"Network: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "print(f\"Mean similarity: {np.mean([d['weight'] for u, v, d in G.edges(data=True)]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SIR Model\n",
    "\n",
    "- Transmission: β × edge weight (similarity)\n",
    "- Recovery: γ (fixed probability per time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sir(G, seed, beta, gamma, max_steps=100):\n",
    "    \"\"\"Run SIR simulation, return S, I, R counts over time.\"\"\"\n",
    "    infected = {seed}\n",
    "    recovered = set()\n",
    "    \n",
    "    S_counts = [G.number_of_nodes() - 1]\n",
    "    I_counts = [1]\n",
    "    R_counts = [0]\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        # Recovery\n",
    "        newly_recovered = set()\n",
    "        for node in infected:\n",
    "            if np.random.random() < gamma:\n",
    "                newly_recovered.add(node)\n",
    "        \n",
    "        infected -= newly_recovered\n",
    "        recovered.update(newly_recovered)\n",
    "        \n",
    "        # Infection\n",
    "        new_infected = set()\n",
    "        for node in infected:\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if neighbor not in infected and neighbor not in recovered:\n",
    "                    if np.random.random() < beta * G[node][neighbor]['weight']:\n",
    "                        new_infected.add(neighbor)\n",
    "        \n",
    "        infected.update(new_infected)\n",
    "        \n",
    "        # Track\n",
    "        S_counts.append(G.number_of_nodes() - len(infected) - len(recovered))\n",
    "        I_counts.append(len(infected))\n",
    "        R_counts.append(len(recovered))\n",
    "        \n",
    "        # Stop if no infected\n",
    "        if len(infected) == 0:\n",
    "            break\n",
    "    \n",
    "    return S_counts, I_counts, R_counts\n",
    "\n",
    "\n",
    "def measure_influence(G, seed, beta, gamma, num_runs=30):\n",
    "    \"\"\"Measure outbreak metrics for a seed node.\"\"\"\n",
    "    final_outbreak_sizes = []\n",
    "    peak_infections = []\n",
    "    epidemic_durations = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        S, I, R = run_sir(G, seed, beta, gamma)\n",
    "        \n",
    "        # Final outbreak size (total recovered at end)\n",
    "        final_outbreak_sizes.append(R[-1])\n",
    "        \n",
    "        # Peak infection\n",
    "        peak_infections.append(max(I))\n",
    "        \n",
    "        # Duration (when epidemic ended)\n",
    "        epidemic_durations.append(len(I))\n",
    "    \n",
    "    return {\n",
    "        'final_outbreak': np.mean(final_outbreak_sizes),\n",
    "        'std_outbreak': np.std(final_outbreak_sizes),\n",
    "        'peak_infection': np.mean(peak_infections),\n",
    "        'duration': np.mean(epidemic_durations),\n",
    "        'attack_rate': np.mean(final_outbreak_sizes) / G.number_of_nodes(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute strength\n",
    "strength = dict(G.degree(weight='weight'))\n",
    "pagerank = nx.pagerank(G, weight='weight')\n",
    "\n",
    "# Select test seeds\n",
    "top_strength = sorted(strength.items(), key=lambda x: x[1], reverse=True)\n",
    "bottom_strength = sorted(strength.items(), key=lambda x: x[1])\n",
    "\n",
    "test_seeds = {\n",
    "    'High Strength': top_strength[0][0],\n",
    "    'Low Strength': bottom_strength[0][0],\n",
    "    'Medium Strength': top_strength[len(top_strength)//2][0],\n",
    "    'Random': np.random.choice(users),\n",
    "}\n",
    "\n",
    "print(\"Test seeds:\")\n",
    "for label, node in test_seeds.items():\n",
    "    print(f\"  {label:15s}: User {node:5d} (strength={strength[node]:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Test Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 0.3\n",
    "GAMMA = 0.1\n",
    "NUM_RUNS = 50\n",
    "\n",
    "print(f\"Running SIR simulations (β={BETA}, γ={GAMMA}, {NUM_RUNS} runs per seed)...\\n\")\n",
    "\n",
    "test_results = {}\n",
    "for label, seed in test_seeds.items():\n",
    "    metrics = measure_influence(G, seed, BETA, GAMMA, NUM_RUNS)\n",
    "    test_results[label] = metrics\n",
    "    print(f\"{label:15s}: outbreak={metrics['final_outbreak']:6.1f} ({metrics['attack_rate']*100:4.1f}%), peak={metrics['peak_infection']:5.1f}\")\n",
    "\n",
    "print(\"\\nObservation: Does high strength → larger outbreaks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize SIR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot S, I, R curves for high vs low strength\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (label, seed) in enumerate([('High Strength', test_seeds['High Strength']), \n",
    "                                      ('Low Strength', test_seeds['Low Strength'])]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Run multiple simulations\n",
    "    all_S, all_I, all_R = [], [], []\n",
    "    for _ in range(30):\n",
    "        S, I, R = run_sir(G, seed, BETA, GAMMA)\n",
    "        all_S.append(S)\n",
    "        all_I.append(I)\n",
    "        all_R.append(R)\n",
    "    \n",
    "    # Average curves\n",
    "    max_len = max(len(s) for s in all_S)\n",
    "    S_padded = [s + [s[-1]] * (max_len - len(s)) for s in all_S]\n",
    "    I_padded = [i + [i[-1]] * (max_len - len(i)) for i in all_I]\n",
    "    R_padded = [r + [r[-1]] * (max_len - len(r)) for r in all_R]\n",
    "    \n",
    "    times = np.arange(max_len)\n",
    "    ax.plot(times, np.mean(S_padded, axis=0), label='S (Susceptible)', linewidth=2)\n",
    "    ax.plot(times, np.mean(I_padded, axis=0), label='I (Infected)', linewidth=2)\n",
    "    ax.plot(times, np.mean(R_padded, axis=0), label='R (Recovered)', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel('Number of Nodes')\n",
    "    ax.set_title(f'SIR Dynamics: {label}')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measure All Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS_PER_NODE = 20\n",
    "\n",
    "print(f\"Measuring influence for all {len(users)} nodes...\")\n",
    "print(f\"β={BETA}, γ={GAMMA}, {NUM_RUNS_PER_NODE} runs per node (5-10 min)\\n\")\n",
    "\n",
    "influence = {}\n",
    "for node in tqdm(users):\n",
    "    metrics = measure_influence(G, node, BETA, GAMMA, NUM_RUNS_PER_NODE)\n",
    "    influence[node] = {\n",
    "        **metrics,\n",
    "        'strength': strength[node],\n",
    "        'pagerank': pagerank[node],\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(influence, orient='index')\n",
    "df.index.name = 'user_id'\n",
    "df = df.sort_values('final_outbreak', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most influential (largest outbreaks):\")\n",
    "print(df.head(10)[['final_outbreak', 'attack_rate', 'peak_infection', 'strength']].to_string())\n",
    "\n",
    "print(\"\\nBottom 10 least influential:\")\n",
    "print(df.tail(10)[['final_outbreak', 'attack_rate', 'peak_infection', 'strength']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "corr_strength = df['final_outbreak'].corr(df['strength'])\n",
    "corr_pagerank = df['final_outbreak'].corr(df['pagerank'])\n",
    "corr_peak = df['peak_infection'].corr(df['strength'])\n",
    "\n",
    "print(\"Correlation with outbreak size:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final outbreak vs Strength:  {corr_strength:7.4f}\")\n",
    "print(f\"Final outbreak vs PageRank:  {corr_pagerank:7.4f}\")\n",
    "print(f\"Peak infection vs Strength:  {corr_peak:7.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "if corr_strength > 0.7:\n",
    "    print(\"  ✓ Strength STRONGLY predicts outbreak size\")\n",
    "elif corr_strength > 0.3:\n",
    "    print(\"  → Strength moderately predicts outbreak size\")\n",
    "else:\n",
    "    print(\"  ✗ Strength weakly predicts outbreak size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(df['strength'], df['final_outbreak'], alpha=0.5, s=30)\n",
    "axes[0].set_xlabel('Strength')\n",
    "axes[0].set_ylabel('Final Outbreak Size')\n",
    "axes[0].set_title(f'Outbreak Size vs Strength (r={corr_strength:.3f})')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].scatter(df['strength'], df['attack_rate'], alpha=0.5, s=30)\n",
    "axes[1].set_xlabel('Strength')\n",
    "axes[1].set_ylabel('Attack Rate (fraction infected)')\n",
    "axes[1].set_title(f'Attack Rate vs Strength')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save influence rankings\n",
    "df.to_csv('../results/similarity_network_sir_influence.csv')\n",
    "print(\"Saved: results/similarity_network_sir_influence.csv\")\n",
    "\n",
    "# Save summary\n",
    "summary = pd.DataFrame([{\n",
    "    'network_type': 'similarity_weighted',\n",
    "    'model': 'SIR',\n",
    "    'num_nodes': G.number_of_nodes(),\n",
    "    'density': nx.density(G),\n",
    "    'beta': BETA,\n",
    "    'gamma': GAMMA,\n",
    "    'metric': 'final_outbreak',\n",
    "    'corr_strength': corr_strength,\n",
    "    'corr_pagerank': corr_pagerank,\n",
    "    'mean_attack_rate': df['attack_rate'].mean(),\n",
    "    'top_node': df.index[0],\n",
    "    'bottom_node': df.index[-1],\n",
    "}])\n",
    "summary.to_csv('../results/similarity_network_sir_summary.csv', index=False)\n",
    "print(\"Saved: results/similarity_network_sir_summary.csv\")\n",
    "\n",
    "print(\"\\nReady for comparison!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
